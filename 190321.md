## PYTHON网络爬虫与信息提取
## 1.1 Requests库
##example1
import requests
r=requests.get("http://www.baidu.com")
print(r.status_code)##200
type(r)##<class 'requests.models.Response'>

##Response对象属性
type(r)
<class 'requests.models.Response'>
r.encoding
'ISO-8859-1'
r.apparent_encoding
'utf-8'
r.encoding='utf-8'##利用此编码替换编码
r.text
##说明：r.encoding 从HTTP header中猜测的相应内容编码方式 如果header中不存在charset 则认为编码为ISO-8859-1
##r.apparent_encoding 从内容中分析出相应的编码方式（备选编码方式） 分析内容得到编码信息
##request库异常
##r.raise_for_status() 如果不是200 产生异常requests.HTTPError

##[！！！爬取网页的通用代码框架！！！]
import requests
def getHTMLText(url):
    try:
        r=requests.get(url,timeout=30)
        r.raise_for_status()##如果状态不是200 引发HTTPError异常
        r.encoding=r.apparent_encoding
        return r.text
    except:
        return "Error"
if __name__=="__main__":
    url="http://www.baidu.com"
    print(getHTMLText(url))

##HTTP协议 Hypetext Transfer Protocol超文本传输协议 基于“请求与相应”模式的、无状态的应用层协议
##URL格式 http://host[:port][path] host：合法的INTERNET主机域名或IP地址 port:端口号，缺省端口为80 path：请求资源的路径
##URL是通过HTTP协议存取资源的Internet路径，一个URL对一个数据资源
##操作 get head post put patch delete

##requests.request(method,url,*kwargs) method:请求方法，对应get/put/post等7种 url:获取页面链接 kwargs:控制参数

## 1.2 网络爬虫“盗亦有道”
#Robots协议

## 1.3网络爬虫实例
#京东商品页面爬虫
import requests
url="https://item.jd.com/27378844116.html"
try:
    r=requests.get(url)
    print(r.raise_for_status())
    r.encoding=r.apparent_encoding
    print(r.rext[:10000])
except:
    print("Fail!")
#没有成功，发现网页不允许爬虫，修改程序如下：
import requests
url="https://item.jd.com/100000986052.html"
try:
    kv={'user-agent':'Mozilla/5.0'}#修改user-agent 模拟为网络访问
    r=requests.get(url,headers=kv)
    print(r.raise_for_status())
    r.encoding=r.apparent_encoding
    print(r.text[:2000])
except:
    print("Fail!")

##百度搜索
>>> import requests
>>> kv={'wd':'Python'}
>>> r=requests.get("http://www.baidu.com/s",params=kv)
>>> r.status_code
200
>>> r.request.url
'http://www.baidu.com/s?wd=Python'
>>> len(r.text)
451074
#规范代码框架  又错了。。。框架就有错，别的就没问题。。。
import requests
keyword="Python"
try:
    kv={'wd':keyword}
    r=requests.get("http://www.baidu.com/s",params=kv)
    print(r.requests.url)
    r.raise_for_status()
    print(len(r.text))
except:
    print("Fail!!")


##网络图片的爬取和存储

